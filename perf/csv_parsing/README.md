# Parsing Benchmarks

## parse_baseline

The goal of this benchmark is to understand and evaluate the performance of parsing CSV using the [csv](https://docs.rs/csv/latest/csv/), and JSON using [json_deserializer](https://github.com/jorgecarleitao/json-deserializer.git) and [simd_json](https://github.com/simd-lite/simd-json).

The code for this is in [parse.rs](./benches/parse.rs). The benchmark is run on data generated by the `gen_data.py` script in this folder.

We first establish a few baseline metrics:
- Reading each file to memory sequentially.
- Reading each file to memory in parallel (using rayon).
- Iterating through every character after reading each file in parallel.

Then we bench mark the following:

- Iterate as a `csv::StringRecord` making a new copy for each record
- Iterate as a `csv::ByteRecord` making a new copy for each record
- Iterate as a `csv::ByteRecord` with a single `ByteRecord` copy (effectively minimizing memory allocations per record).

### Running
```
cargo criterion --message-format=json > test.json;
cat test.json | python3 ../../utils/criterion_to_md.py;
```
- 
### Results

#### MacBook (Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz)

|benchmark|estimate (ms) |lower (ms)|upper (ms)|
|---------|--------|-----|-----|
|parse_baseline/csv_seq_io_baseline|30.68|30.11|31.35|
|parse_baseline/csv_par_io_baseline|29.76|29.65|29.93|
|parse_baseline/json_par_io_baseline|128.93|128.21|129.87|
|parse_baseline/csv_par_baseline_iter_char|29.61|29.41|29.91|
|parse_baseline/json_par_baseline_iter_char|129.95|129.49|130.69|
|parse_baseline/csv_file_reader_string|592.5|580.55|606.46|
|parse_baseline/csv_file_reader_byte|585.21|577.44|593.89|
|parse_baseline/async_io_reader_reference|413.44|406.44|422.55|
|parse_baseline/json_deserializer|4406.45|4310.1|4531.04|
|parse_baseline/simd_json|2750.51|2718.59|2780.92|
|parse_baseline/csv_file_reader_record_reference|167.53|165.44|169.92|

#### Intel(R) Core(TM) i5-10600K CPU @ 4.10GHz

|benchmark|estimate (ms) |lower (ms)|upper (ms)|
|---------|--------|-----|-----|
|parse_baseline/csv_seq_io_baseline|35.4|35.37|35.44|
|parse_baseline/csv_par_io_baseline|34.66|34.61|34.74|
|parse_baseline/json_par_io_baseline|81.24|81.21|81.28|
|parse_baseline/csv_par_baseline_iter_char|34.61|34.54|34.66|
|parse_baseline/json_par_baseline_iter_char|81.08|81.0|81.14|
|parse_baseline/csv_file_reader_string|194.3|194.17|194.42|
|parse_baseline/csv_file_reader_byte|181.45|181.37|181.57|
|parse_baseline/async_io_reader_reference|237.02|236.58|237.5|
|parse_baseline/json_deserializer|1891.68|1878.17|1917.48|
|parse_baseline/simd_json|973.53|973.05|974.07|
|parse_baseline/csv_file_reader_record_reference|113.97|113.94|114.01|
### Conclusions

- StringRecord vs ByteRecord seems to make a marginal difference in this dataset.
- Reusing a ByteRecord makes a huge difference.

## Follow Up Questions

- Does the serde conversion path in the csv crate use a single ByteRecord?
    - Yes
- arrow2 uses a single ByteRecord for inferring schema, but uses a batch of ByteRecords for deserializing into arrow. Can it benefit from a single ByteRecord?
- How does arrow2_convert performance compare, where we use a statically typed schema, and use serde to convert to an intermediate struct?
- There are some complex match expressions in the csv library, and from prior experience these seem to cause a performance hit. Look into using a lookup table for optimization.
    - After further reading this seems to be optimization performed by LLVM:
        - [Example of a neat LLVM optimization](https://www.reddit.com/r/rust/comments/31kras/are_match_statements_constanttime_operations/)

## csv_to_arrow

The goal of this benchmark is to understand and evaluate the performance of parsing CSV to arrow. We evaluate parsing using [arrow2](https://github.com/jorgecarleitao/arrow2) and [arrow2_convert](https://github.com/DataEngineeringLabs/arrow2-convert).
